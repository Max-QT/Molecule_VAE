{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"d:\\Code\\chemical_vae-main\\venv\\lib\\site-packages\\rdkit\\Chem\\PandasTools.py\", line 130, in <module>\n",
      "    if 'display.width' in pd.core.config._registered_options:\n",
      "AttributeError: module 'pandas.core' has no attribute 'config'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  # 使用cpu\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = '2'  # 只显示 warning 和 Error\n",
    "os.chdir(\"Molecule_VAE\")  # 更改工作路径\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# rdkit stuff\n",
    "from rdkit.Chem import AllChem as Chem\n",
    "from rdkit.Chem import PandasTools\n",
    "from rdkit.Chem import Draw\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from chemvae.vae_utils import VAEUtils\n",
    "from chemvae import mol_utils as mu\n",
    "\n",
    "import keras\n",
    "from keras.metrics import categorical_accuracy\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Code\\chemical_vae-main\\venv\\lib\\site-packages\\keras\\models.py:251: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using standarized functions? True\n",
      "Standarization: estimating mu and std values ...done!\n"
     ]
    }
   ],
   "source": [
    "args = {'load_from_dataset':False}\n",
    "# args['dict'] = 'models/server/train_3'  # MP\n",
    "args['dict'] = 'models/server/train_2'  # BP\n",
    "# args['dict'] = 'models/server/train_5'  # PSA\n",
    "# args['dict'] = 'models/server/train_6'  # viscosity\n",
    "# args['dict'] = 'models/server/train_7'  # dielectric_constants\n",
    "\n",
    "# Load a model\n",
    "load_from_dataset = args['load_from_dataset']\n",
    "vae = VAEUtils(directory=args['dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict property\n",
    "if load_from_dataset:\n",
    "    csv_path = args['dict'] + '/val.csv'\n",
    "    df = pd.read_csv(csv_path)\n",
    "    i = 4  # 第几个\n",
    "    smiles = df.iloc[i, 0]\n",
    "else:\n",
    "    smiles = \"[Li+].[O-][N+]([O-])=O\"  # 目前数据集不包含盐，盐的SMILES有\".\"\n",
    "    # smiles = \"OC(C(F)(F)F)C(F)(F)F\"\n",
    "    # smiles = \"CC(O)C(C)O\"\n",
    "    # smiles = \"O=Cc1ccccc1\"\n",
    "\n",
    "smiles_1 = mu.canon_smiles(smiles)  # 规范化smile表达\n",
    "X_1 = vae.smiles_to_hot(smiles_1, canonize_smiles=False)\n",
    "z_1 = vae.encode(X_1)\n",
    "X_r = vae.decode(z_1)\n",
    "\n",
    "print('{:20s} : {}'.format('Input', smiles_1))\n",
    "print('{:20s} : {}'.format('Reconstruction', vae.hot_to_smiles(X_r, strip=True)[0]))\n",
    "\n",
    "print('{:20s} : {}'.format('hot_key', X_1))\n",
    "print('{:20s} : {}'.format('hot_key_Reconstruction', X_r))\n",
    "loss = keras.backend.mean(keras.losses.categorical_crossentropy(tf.convert_to_tensor(X_1), tf.convert_to_tensor(X_r)))\n",
    "sess = tf.Session()\n",
    "print('{:20s} : {}'.format('categorical_crossentropy_loss', sess.run(loss)))\n",
    "\n",
    "accuracy = keras.backend.mean(categorical_accuracy(tf.convert_to_tensor(X_1), tf.convert_to_tensor(X_r)))\n",
    "print(sess.run(categorical_accuracy(tf.convert_to_tensor(X_1), tf.convert_to_tensor(X_r))))\n",
    "print('{:20s} : {}'.format('categorical_accuracy', sess.run(accuracy)))\n",
    "\n",
    "print('{:20s} : {} with norm {:.3f}'.format('Z representation', z_1.shape, np.linalg.norm(z_1)))\n",
    "\n",
    "if load_from_dataset:\n",
    "    df = pd.read_csv(args['dict']+\"/train.csv\")\n",
    "    print('{:20s} : {}'.format('Properties', df.columns.tolist()[1:]))\n",
    "    y_1 = vae.predict_prop_Z(z_1)[0]  # vae.predict_prop_Z()可以实现反归一化预测值\n",
    "    y_p = df.iloc[i, 1:].values.tolist()\n",
    "    print('{:20s} : {}'.format(\"Ground_truth:\", y_p))\n",
    "    print('{:20s} : {}'.format(\"Predict:\", y_1))\n",
    "\n",
    "    loss = tf.sqrt(keras.losses.mean_squared_error(tf.convert_to_tensor(np.array(y_1)), tf.convert_to_tensor(np.array(y_p))))\n",
    "    print('{:20s} : {}'.format('RMSE', sess.run(loss)))\n",
    "\n",
    "    normalize_out = args['dict'] + \"/norm_data.csv\"\n",
    "    df = pd.read_csv(normalize_out)\n",
    "    mean = df['mean']\n",
    "    std = df['std']\n",
    "    y_1 = ((y_1-mean)/std).values.tolist()\n",
    "    y_p = ((y_p-mean)/std).values.tolist()\n",
    "\n",
    "    loss = tf.sqrt(keras.losses.mean_squared_error(tf.convert_to_tensor(np.array(y_1)), tf.convert_to_tensor(np.array(y_p))))\n",
    "    print('{:20s} : {}'.format('RMSE after norm', sess.run(loss)))\n",
    "\n",
    "    relative_loss = tf.abs(tf.convert_to_tensor(np.array(y_1)) - tf.convert_to_tensor(np.array(y_p))) / tf.abs(tf.convert_to_tensor(np.array(y_1)))\n",
    "    print('{:20s} : {}'.format('Relative loss', sess.run(relative_loss)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
