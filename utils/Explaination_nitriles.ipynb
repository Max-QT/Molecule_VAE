{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.chdir('Molecule_VAE')\n",
    "import json\n",
    "import pandas as pd\n",
    "from lime import lime_tabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chemvae.vae_utils import VAEUtils\n",
    "from chemvae import hyperparameters\n",
    "import chemvae.mol_utils as mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wrapper(object):\n",
    "    def __init__(self, model, index=None):\n",
    "        self._model = model\n",
    "        self.index = index\n",
    "\n",
    "    def predict(self, dataset):\n",
    "        \"\"\"\n",
    "        输入是hot格式，不是smiles格式\n",
    "        \"\"\"\n",
    "        if self.index is not None:\n",
    "            # X_1 = self._model.smiles_to_hot(dataset, canonize_smiles=False)\n",
    "            z_1 = self._model.encode(dataset)\n",
    "            y_1 = self._model.predict_prop_Z(z_1)\n",
    "            return y_1[:, self.index]  # 只能选择一个属性进行解释\n",
    "        else:\n",
    "            z_1 = self._model.encode(dataset)\n",
    "            y_1 = self._model.predict_prop_Z(z_1)\n",
    "            return y_1[:]  # 只能选择一个属性进行解释\n",
    "    \n",
    "    def predict_all(self, dataset):\n",
    "        z_1 = self._model.encode(dataset)\n",
    "        y_1 = self._model.predict_prop_Z(z_1)\n",
    "        return y_1  # 预测所有特征结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Code\\chemical_vae-main\\venv\\lib\\site-packages\\keras\\models.py:251: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using standarized functions? True\n",
      "Standarization: estimating mu and std values ...done!\n"
     ]
    }
   ],
   "source": [
    "# 导入VAE model\n",
    "args = {'exp_file': 'exp.json'}\n",
    "# args['directory'] = 'models/server/train_3'  # MP\n",
    "# args['directory'] = 'models/server/train_2'  # BP\n",
    "args['directory'] = 'models/server/train_5'  # PSA\n",
    "# args['directory'] = 'models/server/train_6'  # viscosity\n",
    "# args['directory'] = 'models/server/train_7'  # dielectric_constants\n",
    "\n",
    "if args['directory'] is not None:\n",
    "    args['exp_file'] = os.path.join(args['directory'], args['exp_file'])\n",
    "params = hyperparameters.load_params(args['exp_file'], verbose=False)\n",
    "classes = params['reg_prop_tasks']\n",
    "with open(params['char_file'], 'r', encoding='UTF-8') as f:\n",
    "    CHARS = json.load(f)\n",
    "feature_names = CHARS\n",
    "NCHARS = len(CHARS)\n",
    "CHAR_INDICES = dict((c, i) for i, c in enumerate(CHARS))\n",
    "\n",
    "# 导入数据\n",
    "smiles_train, Y_train = mu.load_smiles_and_data_df(params['data_file'], params['MAX_LEN'],\n",
    "                                                   reg_tasks=params['reg_prop_tasks'],\n",
    "                                                   normalize_out=params[\"data_normalization_out\"])\n",
    "X_train = mu.smiles_to_hot(smiles_train, params['MAX_LEN'], params['PADDING'], CHAR_INDICES, NCHARS)\n",
    "\n",
    "smiles_test, Y_test = mu.load_smiles_and_data_df(params['val_data_file'], params['MAX_LEN'],\n",
    "                                                 reg_tasks=params['reg_prop_tasks'],\n",
    "                                                 normalize_out=params[\"data_normalization_out\"])\n",
    "X_test = mu.smiles_to_hot(smiles_test, params['MAX_LEN'], params['PADDING'], CHAR_INDICES, NCHARS)\n",
    "\n",
    "\n",
    "# 导入模型\n",
    "vae = VAEUtils(directory=args['directory'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "Y_train_one = Y_train[:, index]  # 只能选择一个属性进行解释\n",
    "Y_test_one = Y_test[:, index]  # 只能选择一个属性进行解释\n",
    "model = Wrapper(vae, index)  #模型封装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = lime_tabular.RecurrentTabularExplainer(X_train, \n",
    "                                                   feature_names=feature_names,\n",
    "                                                   discretize_continuous=False,\n",
    "                                                   mode='regression',\n",
    "                                                   feature_selection='none',\n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Properties           : ['PSA']\n",
      "Input                : COCCF\n",
      "Predict:             : [8.30926853]\n",
      "Input                : COCC(F)F\n",
      "Predict:             : [9.18047326]\n",
      "Input                : COCC(F)(F)F\n",
      "Predict:             : [7.9944353]\n"
     ]
    }
   ],
   "source": [
    "# smiles_list = ['COCCC#N', 'CCCCC#N', 'CC(C)CC#N', 'CCCCCC#N', 'CC(C)CCC#N', 'CCCCCCC#N', 'CCC#N', 'CCCC#N', 'CC(C)C#N']  # 所有腈类\n",
    "\n",
    "# smiles_list = ['COCCC#N', 'OCCC#N', 'CCCC#N', 'COCC#N', 'COCC', 'COCCCN', 'COCCC#C', 'CCC#N', 'CC#N', 'COCCF', 'COCC(F)F', 'COCC(F)(F)F']  # MPN\n",
    "smiles_list = ['COCCF', 'COCC(F)F', 'COCC(F)(F)F']  # MPN\n",
    "\n",
    "# smiles_list = ['CCCCC#N', 'CCCCC#C', 'CCCCCN', 'CCCC', 'CCC']  # CCCC#N, CCC#N, CC#N, 都已经有了，不用算  # 戊腈\n",
    "\n",
    "# smiles_list = ['CC(C)CC#N', 'CC(C)C#N', 'CC(C)CC#C', 'CC(C)CCN', 'CC(C)C']  # CCCC#N, CCC#N, CC#N, 'CCC', 都已经有了，不用算  # 异戊腈\n",
    "\n",
    "# smiles_list = ['CCCCCC#N', 'CCCCCC#C', 'CCCCCCN', 'CCCCC']  # CCCCC#N, CCCC#N, CCC#N, CC#N, CCCC, 都已经有了，不用算  # 己腈\n",
    "\n",
    "# smiles_list = ['CC(C)CCC#N', 'CCCCC#N', 'CCCC#N', 'CCC#N', 'CC#N', 'CC(C)CCC#C', 'CC(C)CCCN', 'CC(C)CC', 'CC(C)C']  # 差一个CC(C)CC#N，CC(C)C#N  # 异己腈\n",
    "\n",
    "# smiles_list = ['CCCCCCC#N', 'CCCCCCC#C', 'CCCCCCCN', 'CCCCCC']  # CCCCCC#N, CCCCC#N, CCCC#N, CCC#N, CC#N, CCCCC, 都已经有了，不用算  # 庚腈\n",
    "\n",
    "X_test = mu.smiles_to_hot(smiles_list, params['MAX_LEN'], params['PADDING'], CHAR_INDICES, NCHARS)\n",
    "print('{:20s} : {}'.format('Properties', classes))\n",
    "for index_instance in range(len(smiles_list)):\n",
    "\n",
    "    # 预测值变化\n",
    "    canon_smiles_2 = mu.canon_smiles(smiles_list[index_instance])  # 规范化smile表达\n",
    "    print('{:20s} : {}'.format('Input', canon_smiles_2))\n",
    "\n",
    "    X_2 = vae.smiles_to_hot(canon_smiles_2, canonize_smiles=False)\n",
    "    z_2 = vae.encode(X_2)\n",
    "    y_2 = vae.predict_prop_Z(z_2)[0]\n",
    "    print('{:20s} : {}'.format(\"Predict:\", y_2))\n",
    "\n",
    "    # 重要性变化\n",
    "    smiles_len = len(smiles_list[index_instance])\n",
    "    loc2smiles = {i: s for i,s in enumerate(smiles_list[index_instance])}\n",
    "    # print('SMILES: ', smiles_list[index_instance])\n",
    "    exp = explainer.explain_instance(X_test[index_instance], model.predict)\n",
    "\n",
    "    exp_smiles = []\n",
    "    exp_list = exp.as_list()\n",
    "    element = [\"o\", \"s\", \"S\", \"B\", \"#\", \"I\", \"l\", \"O\", \"H\", \"c\", \"=\", \"n\", \"P\", \"C\", \"F\", \"r\", \"N\", \"+\"]  # 有化学意义的符号，n和s没有对应分子\n",
    "    for item in exp_list:\n",
    "        elem = item[0].split('_')[0]\n",
    "        loc = 119-int(item[0].split('-')[1].split(' ')[0] if elem != '-' else item[0].split('-')[2].split(' ')[0])  # VEA的元素是倒着排的，需要-119\n",
    "        \n",
    "        if loc < smiles_len and loc2smiles[loc]==elem:\n",
    "            exp_smiles.append((loc,elem,item))\n",
    "            # print(item)\n",
    "    exp_smiles.sort(key=lambda x: x[0])\n",
    "    # print(exp_smiles)\n",
    "\n",
    "    # 保存为csv\n",
    "    out_list = []\n",
    "    for a in exp_smiles:\n",
    "        out = [str(a[0]), a[1], str(a[2][1])]\n",
    "        out_list.append(out)\n",
    "    pd.DataFrame(out_list).to_csv(r'utils\\Explaination\\Functional_analysis\\{}-{}-{}.csv'.format(smiles_list[0], classes[0], index_instance))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
